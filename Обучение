1) Для отправки запросов на сайт используется модуль requests
Для обработки информации из запроса (парсинга) используется модуль BeautifulSoup4 (bs4)

Использование: import requests | from bs4 import BeautifulSoup (это класс, используемый для обработки HTML страницы)


2) Далее для удобства используется переменная с ссылкой на сайт, который предстоит парсить
Пример: link = "https://www.google.com"

3) Для того чтобы получить информацию с сайта, мы используем запросы к нему.
Запросы бывают двух видов: GET(незащищённый), POST(с защитой, шифрованием, всеми прелестями)
Для запроса типа GET используется: requests.get(ссылка)
Для POST аналогично: requests.post(ссылка)

Поскольку нам предстоит многоразовая работа с данными из сайта, то их следует передать какой-либо переменной,
 дабы не делать запрос каждый раз.

4) Что мы можем получить из запроса?
    .content - байтовое представление информации по ссылке (используется для изображенией, аудио и так далее)
    .text - HTML-код страницы
    .status_code - Статус ответа на запрос. В зависимости от успеха выполнения - разный код (200 - удачно).

5) Для работы с данными, которые мы достанем из HTML с помощью парсера, используется bs4
А за обработку исходной страницы отвечает парсер - lxml в нашем случае (!только для статической информации!)
Пример конкретной работы с парсером по назначению - #3.py, работает с фэндомом - полностью статический сайт.
Далее BeautifulSoup - BS

Так как для парсинга используется сама HTML страница, то нам потребуется создать объект класса BS и передать в него страницу и парсер.
Пример: soup = BeautifulSoup(html, 'lxml').

По сути, BS хранит в себе теги, а мы будем их отбирать по классам и ID,
использованных в html странице (взаимодействие с браузером необходимо).

6) Процесс парсинга.
6.1 Ищем нужные нам элементы(теги) с помощью .find(название_тега, атрибут_тега) - находит первый подходящий элемент.
К атрибутам относится id или class
    id = " " - указание id элемента
    class_ = " " - указание класса элемента
    attrs = {"атрибут" : "имя"} - используется для нахождения по любым атрибутам, где можно указать название атрибута явно
    (не зная о наличии подходящей переменной в BS).
P.S. Один из параметров можно опустить, если указать только id или класс, будет найден первый тег с таким атрибутом.
Если же опустить второй параметр, то будет найдено первое вхождение данного тега.

6.2 Для того чтобы найти все элементы сразу (а не самый первый), используется .find_all
Данный метод, вернёт нам список найденных тегов, к которым мы сможем обратиться через индексы.

6.3 Посмотрев на #2.py, и попробовав вывести там информацию об user-agent, мы получим python-requests и версию модуля.
То есть, сайт понимает, что запрос к нему приходит с какого-то парсера, поскольку никаких других данных мы не указывали.
У многих сайтов есть защита от парсинга (не знаю, зачем она, возможно от создания огромного кол-ва запросов за раз).
Защита может определить, что запрос был сделан парсером, без указания UserAgent'а.
И чтобы обходить данную защиту, нам надо при запросе показать, что мы настоящий пользователь и передать user-agent значение.

Для указания заголовков запроса (в том числе user-agent) используем:
- header = {'user-agent' : "данные"} (регистр не имеет значения, он приводится автоматически к нижнему)
- При вызове самого запроса, нам уже придётся уведомить сайт о том, что мы имеем какой-то идентификатор: get(link, headers = header)

P.S. Заголовки запроса - куки, useragent, какой протокол используется сервером (http или https) и многое другое
Все заголовки запроса можно посмотреть в браузере: Сеть -> Запрос -> Заголовки.
И да, именно так, можно подменять значения куки файлов, user-agent'а и так далее (привет чекерам слитых баз)