1) Для отправки запросов на сайт используется модуль requests
Для обработки информации из запроса (парсинга) используется модуль BeautifulSoup4 (bs4)

Использование: import requests | from bs4 import BeautifulSoup (это класс, используемый для обработки HTML страницы)


2) Далее для удобства используется переменная с ссылкой на сайт, который предстоит парсить
Пример: link = "https://www.google.com"

3) Для того чтобы получить информацию с сайта, мы используем запросы к нему.
Запросы бывают двух видов: GET(незащищённый), POST(с защитой, шифрованием, всеми прелестями)
Для запроса типа GET используется: requests.get(ссылка)
Для POST аналогично: requests.post(ссылка)

Поскольку нам предстоит многоразовая работа с данными из сайта, то их следует передать какой-либо переменной,
 дабы не делать запрос каждый раз.

4) Что мы можем получить из запроса?
    .content - байтовое представление информации по ссылке (используется для изображенией, аудио и так далее)
    .text - HTML-код страницы
    .status_code - Статус ответа на запрос. В зависимости от успеха выполнения - разный код (200 - удачно).

5) Для работы с данными, которые мы достанем из HTML с помощью парсера, используется bs4
А за обработку исходной страницы отвечает парсер - lxml в нашем случае (!только для статической информации!)
Пример конкретной работы с парсером по назначению - #3.py, работает с фэндомом - полностью статический сайт.
Далее BeautifulSoup - BS

Так как для парсинга используется сама HTML страница, то нам потребуется создать объект класса BS и передать в него страницу и парсер.
Пример: soup = BeautifulSoup(html, 'lxml').

По сути, BS хранит в себе теги, а мы будем их отбирать по классам и ID,
использованных в html странице (взаимодействие с браузером необходимо).

6) Процесс парсинга.
6.1 Ищем нужные нам элементы(теги) с помощью .find(название_тега, атрибут_тега) - находит первый подходящий элемент.
К атрибутам относится id или class
    id = " " - указание id элемента
    class_ = " " - указание класса элемента
    attrs = {"атрибут" : "имя"} - используется для нахождения по любым атрибутам, где можно указать название атрибута явно
    (не зная о наличии подходящей переменной в BS).
P.S. Один из параметров можно опустить, если указать только id или класс, будет найден первый тег с таким атрибутом.
Если же опустить второй параметр, то будет найдено первое вхождение данного тега.

6.2 Какую информацию и как мы можем вытащить из тега:
.text - так мы получаем просто текстовое значение, которое записано внутри тега.
.get('атрибут') - такая инструкция используется, если нам необходимо достать какой-то атрибут из тега.
Например, нам нужно узнать ссылку, с которой подгружается какая-то картинка в соответствующий тег.
Для этого используем .find('image').get('src')

6.3 Для того чтобы найти все элементы сразу (а не самый первый), используется .find_all
Данный метод, вернёт нам список найденных тегов, к которым мы сможем обратиться через индексы.

6.4 Посмотрев на #2.py, и попробовав вывести там информацию об user-agent, мы получим python-requests и версию модуля.
То есть, сайт понимает, что запрос к нему приходит с какого-то парсера, поскольку никаких других данных мы не указывали.
У многих сайтов есть защита от парсинга (не знаю, зачем она, возможно от создания огромного кол-ва запросов за раз).
Защита может определить, что запрос был сделан парсером, без указания UserAgent'а.
И чтобы обходить данную защиту, нам надо при запросе показать, что мы настоящий пользователь и передать user-agent значение.

Для указания заголовков запроса (в том числе user-agent) используем:
- header = {'user-agent' : "данные"} (регистр не имеет значения, он приводится автоматически к нижнему)
- При вызове самого запроса, нам уже придётся уведомить сайт о том, что мы имеем какой-то идентификатор: get(link, headers = header)

P.S. Заголовки запроса - куки, useragent, какой протокол используется сервером (http или https) и многое другое
Все заголовки запроса можно посмотреть в браузере: Сеть -> Запрос -> Заголовки.
И да, именно так, можно подменять значения куки файлов, user-agent'а и так далее (привет чекерам слитых баз)

7) Создание фейкового user-agent.
7.1 Для этого используется модуль fake_useragent.
7.2 Для создания рандомных значений user-agent используем следующую конструкцию: fake_useragent.UserAgent().random
Так мы создаём ua случайного браузера, но можно получить агент конкретного браузера:
 .ie - Internet Explorer,
 .msie - Microsoft Edge (по идее),
 .opera - Opera,
 .chrome или .google - Chrome,
 .firefox - FireFox,
 .safari - Safari и так далее. Данное значение используется вместо метода random.
7.3 Если база данных браузеров устарела (не знаю, сколько надо ждать, но случай очень редкий),
можно использовать метод update для класса UserAgent.

8)
